#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: Main (Build/Test/Publish)

on:
  workflow_call:
    inputs:
      spark:
        description: The Spark version of Spark image.
        required: true
        type: string
        default: 3.3.0
      scala:
        description: The Scala version of Spark image.
        required: true
        type: string
        default: 2.12
      java:
        description: The Java version of Spark image.
        required: true
        type: string
        default: 11
      publish:
        description: Publish the image or not.
        required: false
        type: boolean
        default: false

jobs:
  main:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        spark_version:
          - ${{ inputs.spark }}
        scala_version:
          - ${{ inputs.scala }}
        java_version:
          - ${{ inputs.java }}
        image_suffix: [python3-ubuntu, ubuntu, r-ubuntu, python3-r-ubuntu]
    steps:
      - name: Checkout Spark repository
        uses: actions/checkout@v2
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v1
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Login to GHCR
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Generate tags
        run: |
          TAG=scala${{ matrix.scala_version }}-java${{ matrix.java_version }}-${{ matrix.image_suffix }}

          REPO_OWNER=$(echo "${{ github.repository_owner }}" | tr '[:upper:]' '[:lower:]')
          TEST_REPO=ghcr.io/$REPO_OWNER/spark-docker
          # TODO: Change yikunkero to apache
          REPO=yikunkero
          IMAGE_NAME=spark-1011
          IMAGE_PATH=${{ matrix.spark_version }}/$TAG
          UNIQUE_IMAGE_TAG=${{ matrix.spark_version }}-$TAG

          # Find the tags in version.json according to given path and repo
          IMAGE_TAGS=`./image-meta.py -p $IMAGE_PATH -i $REPO/$IMAGE_NAME`
          TEST_IMAGE_TAGS=`./image-meta.py -p $IMAGE_PATH -i $TEST_REPO/$IMAGE_NAME`

          # Unique image tag in each version: scala2.12-java11-python3-ubuntu
          echo "UNIQUE_IMAGE_TAG=${UNIQUE_IMAGE_TAG}" >> $GITHUB_ENV
          # Test repo: ghcr.io/apache/spark-docker
          echo "TEST_REPO=${TEST_REPO}" >> $GITHUB_ENV
          # Image name: spark
          echo "IMAGE_NAME=${IMAGE_NAME}" >> $GITHUB_ENV
          # Image dockerfile path: 3.3.0/scala2.12-java11-python3-ubuntu
          echo "IMAGE_PATH=${IMAGE_PATH}" >> $GITHUB_ENV
          # IMAGE_TAGS: apache/spark:3.3.0-scala2.12-java11-python3-ubuntu,apache/spark:3.3.0-python3,apache/spark:python3,apache/spark:3.3.0,apache/spark:latest
          echo "IMAGE_TAGS=${IMAGE_TAGS}" >> $GITHUB_ENV
          # TEST_IMAGE_TAGS: ghcr.io/yikun/spark-docker/spark:3.3.0-scala2.12-java11-python3-ubuntu,ghcr.io/yikun/spark-docker/spark:3.3.0-python3
          echo "TEST_IMAGE_TAGS=${TEST_IMAGE_TAGS}" >> $GITHUB_ENV

      - name: Print Image tags
        run: |
          echo "UNIQUE_IMAGE_TAG: "${UNIQUE_IMAGE_TAG}
          echo "TEST_REPO: "${TEST_REPO}
          echo "IMAGE_NAME: "${IMAGE_NAME}
          echo "IMAGE_PATH: "${IMAGE_PATH}
          echo "IMAGE_TAGS: "${IMAGE_TAGS} | xargs -d"," -n1
          echo "TEST_IMAGE_TAGS: "${TEST_IMAGE_TAGS} | xargs -d"," -n1

      - name: Test - Build and push test image
        uses: docker/build-push-action@v2
        with:
          context: ${{ env.IMAGE_PATH }}
          tags: ${{ env.TEST_IMAGE_TAGS }}
          platforms: linux/amd64,linux/arm64

      - name: Test - Checkout Spark repository
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: v${{ matrix.spark_version }}
          path: ${{ github.workspace }}/spark

      - name: Test - Cherry pick commits
        working-directory: ${{ github.workspace }}/spark
        run: |
          # https://github.com/apache/spark/commit/83963828b54bffe99527a004057272bc584cbc26
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' cherry-pick 83963828b54bffe99527a004057272bc584cbc26
          # https://github.com/apache/spark/commit/5ea2b386eb866e20540660cdb6ed43792cb29969
          git -c user.name='Apache Spark Test Account' -c user.email='sparktestacc@gmail.com' cherry-pick 5ea2b386eb866e20540660cdb6ed43792cb29969

      - name: Test - Install Java ${{ inputs.java }}
        uses: actions/setup-java@v1
        with:
          java-version: ${{ matrix.java_version }}

      - name: Test - Cache Scala, SBT and Maven
        uses: actions/cache@v2
        with:
          path: |
            build/apache-maven-*
            build/scala-*
            build/*.jar
            ~/.sbt
          key: build-${{ matrix.spark_version }}-scala${{ matrix.scala_version }}-java${{ matrix.java_version }}

      - name: Test - Cache Coursier local repository
        uses: actions/cache@v2
        with:
          path: ~/.cache/coursier
          key: build-${{ matrix.spark_version }}-scala${{ matrix.scala_version }}-java${{ matrix.java_version }}

      - name: Test - Start minikube
        run: |
          # See more in "Installation" https://minikube.sigs.k8s.io/docs/start/
          curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
          sudo install minikube-linux-amd64 /usr/local/bin/minikube
          # Github Action limit cpu:2, memory: 6947MB, limit to 2U6G for better resource statistic
          minikube start --cpus 2 --memory 6144
      - name: Test - Print K8S pods and nodes info
        run: |
          kubectl get pods -A
          kubectl describe node

      - name: Test - Run Spark on K8S integration test (With driver cpu 0.5, executor cpu 0.2 limited)
        working-directory: ${{ github.workspace }}/spark
        run: |
          kubectl create clusterrolebinding serviceaccounts-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts || true
          eval $(minikube docker-env)

          OPTS="-Pkubernetes -Pkubernetes-integration-tests "
          OPTS+="-Dspark.kubernetes.test.driverRequestCores=0.5 -Dspark.kubernetes.test.executorRequestCores=0.2 "
          OPTS+="-Dspark.kubernetes.test.deployMode=minikube "
          OPTS+="-Dspark.kubernetes.test.imageRepo=${TEST_REPO} -Dspark.kubernetes.test.imageTag=${UNIQUE_IMAGE_TAG} "
          OPTS+="-Dspark.kubernetes.test.jvmImage=${IMAGE_NAME} "

          build/sbt $OPTS 'kubernetes-integration-tests/testOnly -- -z "Run SparkPi"'

          if echo ${{ matrix.image_suffix }} | grep -q "python"; then
            OPTS+="-Dspark.kubernetes.test.pythonImage=${IMAGE_NAME} "
            build/sbt $OPTS 'kubernetes-integration-tests/testOnly -- -z "Run PySpark"'
          fi

          if echo ${{ matrix.image_suffix }} | grep -q "r-"; then
            OPTS+="-Psparkr -Dtest.include.tags=r -Dspark.kubernetes.test.rImage=${IMAGE_NAME} "
            build/sbt $OPTS 'kubernetes-integration-tests/testOnly'
          fi

      - name: Test - Upload Spark on K8S integration tests log files
        if: failure()
        uses: actions/upload-artifact@v2
        with:
          name: spark-on-kubernetes-it-log
          path: "**/target/integration-tests.log"

      - name: Push Image
        if: ${{ inputs.publish }}
        uses: docker/build-push-action@v2
        with:
          context: ${{ env.IMAGE_PATH }}
          push: true
          tags: ${{ env.IMAGE_TAGS }}
          platforms: linux/amd64,linux/arm64

      - name: Image digest
        run: echo ${{ steps.docker_build.outputs.digest }}
