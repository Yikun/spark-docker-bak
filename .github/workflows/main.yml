#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

name: Publish

on:
  push:
    branches:
    - '**'
jobs:
  main:
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 1
      matrix:
        spark_version: [3.3.0]
        scala_version: [2.12]
        java_version: [11]
        image_suffix: [python3-ubuntu] #, ubuntu, r-ubuntu, python3-r-ubuntu]
    steps:
      - name: Checkout Spark repository
        uses: actions/checkout@v2
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v1
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
      - name: Login to DockerHub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Login to GHCR
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Generate tags
        run: |
          BASE_PATH=${{ matrix.spark_version }}/scala${{ matrix.scala_version }}-java${{ matrix.java_version }}
          REPO=ghcr.io/yikun/spark-docker/spark-0919
          IMAGE_PATH=$BASE_PATH-${{ matrix.image_suffix }}
          # Find the tags in version.json according to given path and repo
          IMAGE_TAGS=`./image-meta.py -p $IMAGE_PATH -r $REPO`

          echo "IMAGE_PATH=${IMAGE_PATH}" >> $GITHUB_ENV
          echo "IMAGE_TAGS=${IMAGE_TAGS}" >> $GITHUB_ENV

      - name: Print Image tags
        run: |
          echo ${IMAGE_PATH}
          echo ${IMAGE_TAGS} | xargs -d"," -n1

      # - name: Build image
      #   uses: docker/build-push-action@v2
      #   with:
      #     context: ${{ env.IMAGE_PATH }}
      #     push: true
      #     tags: ${{ env.IMAGE_TAGS }}
      #     platforms: linux/amd64,linux/arm64

      - name: Test - Checkout Spark repository
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
          repository: apache/spark
          ref: v${{ matrix.spark_version }}
          path: ${{ github.workspace }}/spark

      - name: Test - Install Java ${{ inputs.java }}
        uses: actions/setup-java@v1
        with:
          java-version: ${{ matrix.java_version }}
      - name: Test - Start minikube
        run: |
          # See more in "Installation" https://minikube.sigs.k8s.io/docs/start/
          curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
          sudo install minikube-linux-amd64 /usr/local/bin/minikube
          # Github Action limit cpu:2, memory: 6947MB, limit to 2U6G for better resource statistic
          minikube start --cpus 2 --memory 6144
      - name: Test - Print K8S pods and nodes info
        run: |
          kubectl get pods -A
          kubectl describe node

      - name: Test - Run Spark on K8S integration test
        working-directory: ${{ github.workspace }}/spark
        run: |
          # Prepare PV test
          PVC_TMP_DIR=$(mktemp -d)
          export PVC_TESTS_HOST_PATH=$PVC_TMP_DIR
          export PVC_TESTS_VM_PATH=$PVC_TMP_DIR
          minikube mount ${PVC_TESTS_HOST_PATH}:${PVC_TESTS_VM_PATH} --gid=0 --uid=185 &
          kubectl create clusterrolebinding serviceaccounts-cluster-admin --clusterrole=cluster-admin --group=system:serviceaccounts || true
          eval $(minikube docker-env)
          # - Exclude Volcano test (-Pvolcano), batch jobs need more CPU resource
          build/sbt -Pkubernetes -Pkubernetes-integration-tests -Dspark.kubernetes.test.namespace=default \
            -Dspark.kubernetes.test.driverRequestCores=0.5 -Dspark.kubernetes.test.executorRequestCores=0.2 \
            -Dspark.kubernetes.test.deployMode=minikube -Dspark.kubernetes.test.imageRepo=ghcr.io/yikun/spark-docker \
            -Dspark.kubernetes.test.imageTag=3.3.0 -Dspark.kubernetes.test.jvmImage=spark-0919 \
            -Dspark.kubernetes.test.pythonImage=spark-0919 "kubernetes-integration-tests/runIts"

      - name: Test - Upload Spark on K8S integration tests log files
        if: failure()
        uses: actions/upload-artifact@v2
        with:
          name: spark-on-kubernetes-it-log
          path: "**/target/integration-tests.log"

      - name: Push (Spark + PySpark)
        uses: docker/build-push-action@v2
        with:
          context: ${{ env.IMAGE_PATH }}
          push: true
          tags: ${{ env.IMAGE_TAGS }}
          platforms: linux/amd64,linux/arm64

      - name: Image digest
        run: echo ${{ steps.docker_build.outputs.digest }}
